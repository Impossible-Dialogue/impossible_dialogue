{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from email.headerregistry import AddressHeader\n",
    "from cv2 import kmeans\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "import mpl_toolkits.mplot3d as m3d\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "import math\n",
    "\n",
    "\n",
    "class WavefrontGroup:\n",
    "    def __init__(self, name='default'):\n",
    "        self.name = name               # group name\n",
    "        # vertices as an Nx3 or Nx6 array (per vtx colors)\n",
    "        self.vertices = []\n",
    "        self.normals = []                 # normals\n",
    "        self.texcoords = []                 # texture coordinates\n",
    "        # M*Nv*3 array, Nv=# of vertices, stored as vid,tid,nid (-1 for N/A)\n",
    "        self.polygons = []\n",
    "\n",
    "\n",
    "class WavefrontOBJ:\n",
    "    def __init__(self, default_mtl='default_mtl'):\n",
    "        self.path = None               # path of loaded object\n",
    "        self.mtllibs = []                 # .mtl files references via mtllib\n",
    "        self.mtls = [default_mtl]    # materials referenced\n",
    "        self.mtlid = []                 # indices into self.mtls for each polygon\n",
    "        # vertices as an Nx3 or Nx6 array (per vtx colors)\n",
    "        self.vertices = []\n",
    "        self.normals = []                 # normals\n",
    "        self.texcoords = []                 # texture coordinates\n",
    "        # M*Nv*3 array, Nv=# of vertices, stored as vid,tid,nid (-1 for N/A)\n",
    "        self.polygons = []\n",
    "        self.groups = []                 # Groups\n",
    "\n",
    "\n",
    "def load_obj(filename: str, default_mtl='default_mtl', triangulate=False) -> WavefrontOBJ:\n",
    "    \"\"\"Reads a .obj file from disk and returns a WavefrontOBJ instance\n",
    "\n",
    "    Handles only very rudimentary reading and contains no error handling!\n",
    "\n",
    "    Does not handle:\n",
    "        - relative indexing\n",
    "        - subobjects or groups\n",
    "        - lines, splines, beziers, etc.\n",
    "    \"\"\"\n",
    "    # parses a vertex record as either vid, vid/tid, vid//nid or vid/tid/nid\n",
    "    # and returns a 3-tuple where unparsed values are replaced with -1\n",
    "    def parse_vertex(vstr):\n",
    "        vals = vstr.split('/')\n",
    "        vid = int(vals[0])-1\n",
    "        tid = int(vals[1])-1 if len(vals) > 1 and vals[1] else -1\n",
    "        nid = int(vals[2])-1 if len(vals) > 2 else -1\n",
    "        return (vid, tid, nid)\n",
    "\n",
    "    with open(filename, 'r') as objf:\n",
    "        obj = WavefrontOBJ(default_mtl=default_mtl)\n",
    "        obj.path = filename\n",
    "        cur_mat = obj.mtls.index(default_mtl)\n",
    "        cur_group = WavefrontGroup()\n",
    "\n",
    "        for line in objf:\n",
    "            toks = line.split()\n",
    "            if not toks:\n",
    "                continue\n",
    "            if toks[0] == 'g':\n",
    "                cur_group = WavefrontGroup(name=toks[1])\n",
    "                obj.groups.append(cur_group)\n",
    "            if toks[0] == 'v':\n",
    "                cur_group.vertices.append([float(v) for v in toks[1:]])\n",
    "            elif toks[0] == 'vn':\n",
    "                cur_group.normals.append([float(v) for v in toks[1:]])\n",
    "            elif toks[0] == 'vt':\n",
    "                cur_group.texcoords.append([float(v) for v in toks[1:]])\n",
    "            elif toks[0] == 'f':\n",
    "                poly = [parse_vertex(vstr) for vstr in toks[1:]]\n",
    "                if triangulate:\n",
    "                    for i in range(2, len(poly)):\n",
    "                        obj.mtlid.append(cur_mat)\n",
    "                        cur_group.polygons.append(\n",
    "                            (poly[0], poly[i-1], poly[i]))\n",
    "                else:\n",
    "                    obj.mtlid.append(cur_mat)\n",
    "                    cur_group.polygons.append(poly)\n",
    "            elif toks[0] == 'mtllib':\n",
    "                obj.mtllibs.append(toks[1])\n",
    "            elif toks[0] == 'usemtl':\n",
    "                if toks[1] not in obj.mtls:\n",
    "                    obj.mtls.append(toks[1])\n",
    "                cur_mat = obj.mtls.index(toks[1])\n",
    "        return obj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(data):\n",
    "    # Calculate the mean of the points, i.e. the 'center' of the cloud\n",
    "    datamean = data.mean(axis=0)\n",
    "    print('mean ' + str(datamean))\n",
    "\n",
    "    # PCA to generate a line representation for each tube\n",
    "    mu = data.mean(0)\n",
    "    C = np.cov(data - mu, rowvar=False)\n",
    "    d, u = np.linalg.eigh(C)\n",
    "    U = u.T[::-1]\n",
    "\n",
    "    # Project points onto the principle axes\n",
    "    Z = np.dot(data - mu, U.T)\n",
    "    print('min Z ' + str(Z.min()))\n",
    "    print('max Z ' + str(Z.max()))\n",
    "\n",
    "    return Z, U, mu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(data):\n",
    "    model = DBSCAN(eps=0.05, min_samples=9)\n",
    "\n",
    "    # fit model and predict clusters\n",
    "    labels = model.fit_predict(data)\n",
    "\n",
    "    # retrieve unique clusters\n",
    "    clusters = unique(labels)\n",
    "\n",
    "    if False:\n",
    "        x2 = Z.T[0]\n",
    "        y2 = Z.T[1]\n",
    "\n",
    "        # create scatter plot for samples from each cluster\n",
    "        for cluster in clusters:\n",
    "            # get row indexes for samples with this cluster\n",
    "            row_ix = where(labels == cluster)\n",
    "            # create scatter of these samples\n",
    "            plt.scatter(x2[row_ix], y2[row_ix], s=3)\n",
    "        # show the plot\n",
    "        plt.rcParams['figure.figsize'] = [25, 5]\n",
    "        plt.axis('scaled')\n",
    "        plt.show()\n",
    "\n",
    "    # print('labels ' + str(labels))\n",
    "    # print('clusters ' + str(clusters))\n",
    "    print('# of clusters ' + str(clusters.size))\n",
    "\n",
    "    return clusters, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "        self.next = None\n",
    "\n",
    "    def distance(self, other_node):\n",
    "        return np.linalg.norm(self.p - other_node.p)\n",
    "\n",
    "\n",
    "def createLineSegments(data, clusters, labels):\n",
    "    # First create an unordered list of nodes, one per cluster with using the mean of the cluster points.\n",
    "    nodes = []\n",
    "    for cluster in clusters:\n",
    "        row_ix = where(labels == cluster)\n",
    "        cluster_points = data[row_ix]\n",
    "        node = Node(p=cluster_points.mean(axis=0))\n",
    "        nodes.append(node)\n",
    "\n",
    "    # Connect nodes to line segments starting from a random node and extending in both directions the result is a start and an end node connected in a linked list\n",
    "    start = nodes.pop()\n",
    "    end = start\n",
    "    while len(nodes) > 0:\n",
    "        min_node = None\n",
    "        min_dist = float('inf')\n",
    "        closest_to_start = True\n",
    "\n",
    "        for node in nodes:\n",
    "            start_dist = start.distance(node)\n",
    "            end_dist = end.distance(node)\n",
    "\n",
    "            if start_dist < min_dist and start_dist <= end_dist:\n",
    "                min_dist = start_dist\n",
    "                min_node = node\n",
    "                closest_to_start = True\n",
    "\n",
    "            if end_dist < min_dist and end_dist < start_dist:\n",
    "                min_dist = end_dist\n",
    "                min_node = node\n",
    "                closest_to_start = False\n",
    "\n",
    "        if closest_to_start:\n",
    "            min_node.next = start\n",
    "            start = min_node\n",
    "        else:\n",
    "            end.next = min_node\n",
    "            end = min_node\n",
    "\n",
    "        nodes.remove(min_node)\n",
    "\n",
    "    return start\n",
    "\n",
    "\n",
    "def lineSegmentsLength(start):\n",
    "    n = start\n",
    "    total_length = 0\n",
    "    while True:\n",
    "        if n.next == None:\n",
    "            break\n",
    "        total_length = total_length + n.distance(n.next)\n",
    "        n = n.next\n",
    "    return total_length\n",
    "\n",
    "# Generate LED positions by tracing along the line segments until the correct number of LED are created\n",
    "def traceLineSegments(start, num_leds, offset, led_dist):\n",
    "    segment_points = []\n",
    "    fraction = 0\n",
    "    n = start\n",
    "    dist = offset\n",
    "    for i in range(num_leds):\n",
    "        new_fraction = fraction + dist / n.distance(n.next)\n",
    "        while new_fraction >= 1:\n",
    "            # Move to next segment\n",
    "            dist = dist - (1 - fraction) * n.distance(n.next)\n",
    "            n = n.next\n",
    "            fraction = 0\n",
    "            new_fraction = dist / n.distance(n.next)\n",
    "\n",
    "        fraction = new_fraction\n",
    "        p = n.p + fraction * (n.next.p - n.p)\n",
    "        segment_points.append(p)\n",
    "        dist = led_dist\n",
    "\n",
    "    return np.array(segment_points)\n",
    "\n",
    "def generateLEDPositions(data, clusters, labels):\n",
    "    LEDS_PER_METER = 60  # eg. 300LEDs/16.4ft\n",
    "    LED_DIST = 1 / LEDS_PER_METER\n",
    "    LED_START_OFFSET = 0.1  # 10cm offset from that start of a segment\n",
    "    LED_END_OFFSET = 0.1\n",
    "\n",
    "    segments = createLineSegments(data, clusters, labels)\n",
    "    length = lineSegmentsLength(segments)\n",
    "    length = length - LED_START_OFFSET - LED_END_OFFSET\n",
    "    num_leds = math.floor(length * LEDS_PER_METER)\n",
    "    points = traceLineSegments(segments, num_leds, LED_START_OFFSET, LED_DIST)\n",
    "    return points, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareData(group):\n",
    "    x = np.array([v[0] for v in group.vertices])\n",
    "    y = np.array([v[1] for v in group.vertices])\n",
    "    z = np.array([v[2] for v in group.vertices])\n",
    "\n",
    "    data = np.concatenate((x[:, np.newaxis],\n",
    "                        y[:, np.newaxis],\n",
    "                        z[:, np.newaxis]),\n",
    "                        axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotSegment(data, clusters, labels, segment_points):\n",
    "    Z, U, mu = PCA(data)\n",
    "    x2 = Z.T[0]\n",
    "    y2 = Z.T[1]\n",
    "\n",
    "    # create scatter plot for samples from each cluster\n",
    "    for cluster in clusters:\n",
    "        # get row indexes for samples with this cluster\n",
    "        row_ix = where(labels == cluster)\n",
    "        # create scatter of these samples\n",
    "        plt.scatter(x2[row_ix], y2[row_ix], s=3)\n",
    "\n",
    "    # create scatter plot for line segments\n",
    "    L = np.dot(segment_points - mu, U.T)\n",
    "    xl2 = L.T[0]\n",
    "    yl2 = L.T[1]\n",
    "    plt.scatter(xl2, yl2, s=3)\n",
    "\n",
    "    # show the plot\n",
    "    plt.rcParams['figure.figsize'] = [25, 5]\n",
    "    plt.axis('scaled')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: Body1\n",
      "# of clusters 4\n",
      "Processing: Body2\n",
      "# of clusters 7\n",
      "Processing: Body3\n",
      "# of clusters 9\n",
      "Processing: Body4\n",
      "# of clusters 9\n",
      "Processing: Body5\n",
      "# of clusters 9\n",
      "Processing: Body6\n",
      "# of clusters 4\n",
      "Processing: Body7\n",
      "# of clusters 7\n",
      "Processing: Body8\n",
      "# of clusters 5\n",
      "Processing: Body9\n",
      "# of clusters 9\n",
      "Processing: Body10\n",
      "# of clusters 9\n",
      "Processing: Body11\n",
      "# of clusters 5\n",
      "Processing: Body12\n",
      "# of clusters 9\n",
      "Processing: Body13\n",
      "# of clusters 5\n",
      "Processing: Body14\n",
      "# of clusters 5\n",
      "Processing: Body15\n",
      "# of clusters 5\n",
      "Processing: Body16\n",
      "# of clusters 9\n",
      "Processing: Body17\n",
      "# of clusters 7\n",
      "Processing: Body18\n",
      "# of clusters 9\n",
      "Processing: Body19\n",
      "# of clusters 4\n",
      "Processing: Body20\n",
      "# of clusters 9\n",
      "Processing: Body21\n",
      "# of clusters 9\n",
      "Processing: Body22\n",
      "# of clusters 5\n",
      "Processing: Body23\n",
      "# of clusters 5\n",
      "Processing: Body24\n",
      "# of clusters 7\n",
      "Processing: Body25\n",
      "# of clusters 9\n",
      "Processing: Body26\n",
      "# of clusters 5\n",
      "Processing: Body27\n",
      "# of clusters 5\n",
      "Processing: Body28\n",
      "# of clusters 9\n",
      "Processing: Body29\n",
      "# of clusters 4\n",
      "Processing: Body30\n",
      "# of clusters 9\n",
      "Processing: Body31\n",
      "# of clusters 7\n",
      "Processing: Body32\n",
      "# of clusters 9\n",
      "Processing: Body33\n",
      "# of clusters 5\n",
      "Processing: Body34\n",
      "# of clusters 4\n",
      "Processing: Body35\n",
      "# of clusters 4\n",
      "Processing: Body36\n",
      "# of clusters 7\n",
      "Processing: Body37\n",
      "# of clusters 7\n",
      "Processing: Body38\n",
      "# of clusters 4\n",
      "Processing: Body39\n",
      "# of clusters 4\n",
      "Processing: Body40\n",
      "# of clusters 4\n",
      "Processing: Body41\n",
      "# of clusters 4\n",
      "Processing: Body42\n",
      "# of clusters 4\n",
      "Processing: Body43\n",
      "# of clusters 7\n",
      "Processing: Body44\n",
      "# of clusters 4\n",
      "Processing: Body45\n",
      "# of clusters 9\n",
      "Processing: Body46\n",
      "# of clusters 9\n",
      "Processing: Body47\n",
      "# of clusters 9\n",
      "Processing: Body48\n",
      "# of clusters 7\n",
      "Processing: Body49\n",
      "# of clusters 4\n",
      "Processing: Body50\n",
      "# of clusters 5\n",
      "Processing: Body51\n",
      "# of clusters 9\n",
      "Processing: Body52\n",
      "# of clusters 9\n",
      "Processing: Body53\n",
      "# of clusters 4\n",
      "Processing: Body54\n",
      "# of clusters 9\n",
      "Processing: Body55\n",
      "# of clusters 9\n",
      "Processing: Body56\n",
      "# of clusters 7\n",
      "Processing: Body57\n",
      "# of clusters 5\n",
      "Processing: Body58\n",
      "# of clusters 9\n",
      "Processing: Body59\n",
      "# of clusters 7\n",
      "Processing: Body60\n",
      "# of clusters 9\n",
      "Processing: Body61\n",
      "# of clusters 4\n",
      "Processing: Body62\n",
      "# of clusters 9\n",
      "Processing: Body63\n",
      "# of clusters 7\n",
      "Processing: Body64\n",
      "# of clusters 9\n",
      "Processing: Body65\n",
      "# of clusters 9\n",
      "Processing: Body66\n",
      "# of clusters 5\n",
      "Processing: Body67\n",
      "# of clusters 4\n",
      "Processing: Body68\n",
      "# of clusters 4\n",
      "Processing: Body69\n",
      "# of clusters 5\n",
      "Processing: Body70\n",
      "# of clusters 9\n",
      "Processing: Body71\n",
      "# of clusters 9\n",
      "Processing: Body72\n",
      "# of clusters 4\n",
      "Processing: Body73\n",
      "# of clusters 7\n",
      "Processing: Body74\n",
      "# of clusters 9\n",
      "Processing: Body75\n",
      "# of clusters 9\n",
      "Processing: Body76\n",
      "# of clusters 5\n",
      "Processing: Body77\n",
      "# of clusters 4\n",
      "Processing: Body78\n",
      "# of clusters 9\n",
      "Processing: Body79\n",
      "# of clusters 5\n",
      "Processing: Body80\n",
      "# of clusters 5\n",
      "Processing: Body81\n",
      "# of clusters 9\n",
      "Processing: Body82\n",
      "# of clusters 9\n",
      "Processing: Body83\n",
      "# of clusters 4\n",
      "Processing: Body84\n",
      "# of clusters 7\n",
      "Processing: Body85\n",
      "# of clusters 4\n",
      "Processing: Body86\n",
      "# of clusters 4\n",
      "Processing: Body87\n",
      "# of clusters 9\n",
      "Processing: Body88\n",
      "# of clusters 9\n",
      "Processing: Body89\n",
      "# of clusters 5\n",
      "Processing: Body90\n",
      "# of clusters 9\n",
      "Processing: Body91\n",
      "# of clusters 9\n",
      "Processing: Body92\n",
      "# of clusters 5\n",
      "Processing: Body93\n",
      "# of clusters 4\n",
      "Processing: Body94\n",
      "# of clusters 7\n",
      "Processing: Body95\n",
      "# of clusters 9\n",
      "Processing: Body96\n",
      "# of clusters 5\n",
      "Processing: Body97\n",
      "# of clusters 9\n",
      "Processing: Body98\n",
      "# of clusters 9\n",
      "Processing: Body99\n",
      "# of clusters 7\n",
      "Processing: Body100\n",
      "# of clusters 9\n",
      "Processing: Body101\n",
      "# of clusters 9\n",
      "Processing: Body102\n",
      "# of clusters 4\n",
      "Processing: Body103\n",
      "# of clusters 5\n",
      "Processing: Body104\n",
      "# of clusters 4\n",
      "Processing: Body105\n",
      "# of clusters 5\n",
      "Processing: Body106\n",
      "# of clusters 4\n",
      "Processing: Body107\n",
      "# of clusters 9\n",
      "Processing: Body108\n",
      "# of clusters 9\n",
      "Processing: Body109\n",
      "# of clusters 9\n",
      "Processing: Body110\n",
      "# of clusters 9\n",
      "Processing: Body111\n",
      "# of clusters 9\n",
      "Processing: Body112\n",
      "# of clusters 9\n",
      "Processing: Body113\n",
      "# of clusters 4\n",
      "Processing: Body114\n",
      "# of clusters 9\n",
      "Processing: Body115\n",
      "# of clusters 5\n",
      "Processing: Body116\n",
      "# of clusters 5\n",
      "Processing: Body117\n",
      "# of clusters 5\n",
      "Processing: Body118\n",
      "# of clusters 4\n",
      "Processing: Body119\n",
      "# of clusters 5\n",
      "Processing: Body120\n",
      "# of clusters 5\n",
      "Processing: Body121\n",
      "# of clusters 9\n",
      "Processing: Body122\n",
      "# of clusters 5\n",
      "Processing: Body123\n",
      "# of clusters 7\n",
      "Processing: Body124\n",
      "# of clusters 4\n",
      "Processing: Body125\n",
      "# of clusters 9\n",
      "Processing: Body126\n",
      "# of clusters 9\n",
      "Processing: Body127\n",
      "# of clusters 5\n",
      "Processing: Body128\n",
      "# of clusters 5\n",
      "Processing: Body129\n",
      "# of clusters 9\n",
      "Processing: Body130\n",
      "# of clusters 5\n",
      "Processing: Body131\n",
      "# of clusters 4\n",
      "Processing: Body132\n",
      "# of clusters 9\n",
      "Processing: Body133\n",
      "# of clusters 5\n",
      "Processing: Body134\n",
      "# of clusters 7\n",
      "Processing: Body135\n",
      "# of clusters 9\n",
      "Processing: Body136\n",
      "# of clusters 5\n",
      "Processing: Body137\n",
      "# of clusters 5\n",
      "Processing: Body138\n",
      "# of clusters 7\n",
      "Processing: Body139\n",
      "# of clusters 7\n",
      "Processing: Body140\n",
      "# of clusters 9\n",
      "Processing: Body141\n",
      "# of clusters 7\n",
      "Processing: Body142\n",
      "# of clusters 4\n",
      "Processing: Body143\n",
      "# of clusters 9\n",
      "Processing: Body144\n",
      "# of clusters 9\n",
      "Processing: Body145\n",
      "# of clusters 9\n",
      "Processing: Body146\n",
      "# of clusters 4\n",
      "Processing: Body147\n",
      "# of clusters 7\n",
      "Processing: Body148\n",
      "# of clusters 9\n",
      "Processing: Body149\n",
      "# of clusters 5\n",
      "Processing: Body150\n",
      "# of clusters 9\n",
      "Processing: Body151\n",
      "# of clusters 7\n",
      "Processing: Body152\n",
      "# of clusters 5\n",
      "Processing: Body153\n",
      "# of clusters 9\n",
      "Processing: Body154\n",
      "# of clusters 4\n",
      "Processing: Body155\n",
      "# of clusters 4\n",
      "Processing: Body156\n",
      "# of clusters 9\n",
      "Processing: Body157\n",
      "# of clusters 9\n",
      "Processing: Body158\n",
      "# of clusters 5\n",
      "Processing: Body159\n",
      "# of clusters 5\n",
      "Processing: Body160\n",
      "# of clusters 7\n",
      "Processing: Body1:1\n",
      "# of clusters 19\n",
      "Processing: Body2:1\n",
      "# of clusters 13\n",
      "Processing: Body3:1\n",
      "# of clusters 26\n",
      "Processing: Body4:1\n",
      "# of clusters 26\n",
      "Processing: Body5:1\n",
      "# of clusters 17\n",
      "Processing: Body10:1\n",
      "# of clusters 22\n",
      "Processing: Body11:1\n",
      "# of clusters 22\n",
      "Processing: Body12:1\n",
      "# of clusters 19\n",
      "Processing: Body13:1\n",
      "# of clusters 19\n",
      "Processing: Body14:1\n",
      "# of clusters 19\n"
     ]
    }
   ],
   "source": [
    "class Segment():\n",
    "    def __init__(self, name, points, length):\n",
    "        self.name = name\n",
    "        self.points = points\n",
    "        self.num_leds = points.shape[0]\n",
    "        self.length = length\n",
    "\n",
    "scene = load_obj('led_components.obj')\n",
    "segments = []\n",
    "for group in scene.groups:\n",
    "    print('Processing: ' + group.name)\n",
    "    data = prepareData(group)\n",
    "    clusters, labels = cluster(data)\n",
    "    points, length = generateLEDPositions(data, clusters, labels)\n",
    "    segments.append(Segment(group.name, points, length))\n",
    "    # plotSegment(data, clusters, labels, segment.points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 1\n",
    "json_dict = {\n",
    "    'total_num_leds': 0,\n",
    "    'total_length': 0,\n",
    "    'led_segments': []\n",
    "}\n",
    "\n",
    "total_num_leds = 0\n",
    "total_length = 0\n",
    "for segment in segments:\n",
    "    total_num_leds = total_num_leds + segment.num_leds\n",
    "    total_length = total_length + segment.length\n",
    "json_dict['total_num_leds'] = total_num_leds\n",
    "json_dict['total_length'] = total_length\n",
    "\n",
    "for segment in segments:\n",
    "    json_dict['led_segments'].append(\n",
    "        {\n",
    "            'uid': uid,\n",
    "            'name': segment.name,\n",
    "            'num_leds': segment.num_leds,\n",
    "            'length': segment.length,\n",
    "            'led_positions': segment.points.tolist()\n",
    "        })\n",
    "    uid = uid + 1\n",
    "\n",
    "with open('led_config.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(json_dict, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
